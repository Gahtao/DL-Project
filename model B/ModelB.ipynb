{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e25d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb64e7f",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa5d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = torch.as_tensor(np.load(img_path))\n",
    "        image = image.unsqueeze(0)\n",
    "        label = one_hot(torch.as_tensor(self.img_labels.iloc[idx, 1]-1),5).unsqueeze(0).to(torch.float)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18fe84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = CustomImageDataset(\"preprocessed/val/labels.csv\", \"preprocessed/val/specs\")\n",
    "train_set = CustomImageDataset(\"preprocessed/train/labels.csv\", \"preprocessed/train/specs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4fa11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd85b8",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a2647",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "1. **Convolutional Layers:**\n",
    "   - `self.conv1`: The first convolutional layer takes the input (RGB images) and produces feature maps with 16 output channels. The kernel size is set to 3 and padding needs to be 1 to keep the input dimension.\n",
    "   - `self.conv2`: The second convolutional layer takes the output of the first after the activation funtion and pooling are applied and produces a feature map with 32 channels, and has the the same kernel size and padding (3 and 1).\n",
    "   - `self.conv3`: The third convolutional layer further increases the number of output channels to 64.\n",
    "\n",
    "2. **Activation and Pooling:**\n",
    "   - `self.relu`: Rectified Linear Unit (ReLU) activation function is applied after each convolutional layer to introduce non-linearity.\n",
    "   - `self.pool`: Max-pooling layer with a kernel size of 2 and a stride of 2 is used to downsample the spatial dimensions.\n",
    "\n",
    "3. **Fully Connected Layers:**\n",
    "   - `self.fc1`: The first fully connected layer takes the flattened output from the last convolutional layer and maps it to 64 units.\n",
    "   - `self.fc2`: The final fully connected layer maps the 64 units to the output space with 10 units, corresponding to the number of classes in CIFAR-10.\n",
    "\n",
    "### Forward Pass\n",
    "The `forward` method defines the forward pass of the model. It specifies how input data flows through the layers to produce the final output. Convolutional and pooling layers are followed by activation functions, and the fully connected layers provide the classification logits. Note that the same relu and pooling layers are used in several parts. That is ok as these layers do not have parameters and are only applying the same function to any input, so no separate layers are needed.\n",
    "\n",
    "This simple CNN serves as a starting point for image classification tasks and can be further customized or extended for more complex problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2af05",
   "metadata": {},
   "source": [
    "Channel = 1 \n",
    "\n",
    "Padding = 2 (because dimension is 2) #only use if moving your filter would move you outside the convolution, so it's not mandatory\n",
    "\n",
    "filter size = width, height and channels (depends on the input size)\n",
    "\n",
    "Stride = how much youÂ´d have overlap in your steps\n",
    "\n",
    "Input first layer = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "737c9bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e330ea54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device set to: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc92db29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.75734079e-05, 8.10304118e-05, 8.59528300e-05, ...,\n",
       "        2.56379972e-05, 3.38812315e-05, 1.03176928e-04],\n",
       "       [4.83323238e-05, 1.34489237e-04, 6.52731978e-05, ...,\n",
       "        2.19504233e-04, 3.46366840e-04, 2.22038609e-04],\n",
       "       [1.51195025e-04, 2.51585559e-04, 4.53107874e-04, ...,\n",
       "        2.88863026e-04, 5.89006930e-04, 3.79063480e-04],\n",
       "       ...,\n",
       "       [4.35408722e-08, 9.69061347e-08, 1.73314916e-07, ...,\n",
       "        1.13678993e-07, 1.68862286e-07, 1.28463967e-07],\n",
       "       [5.15285414e-08, 1.03113841e-07, 1.11438155e-07, ...,\n",
       "        1.39387069e-07, 2.19962430e-07, 1.03649207e-07],\n",
       "       [6.13314484e-08, 1.19523705e-07, 1.11718748e-07, ...,\n",
       "        1.14359693e-07, 1.10422150e-07, 5.54369883e-08]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(f\"preprocessed/train/specs/{os.listdir('preprocessed/train/specs')[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f560a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necesary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540c3fb",
   "metadata": {},
   "source": [
    "## CNN specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "168bce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.dropout_prob = 0.2\n",
    "        \n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Third Convolutional Layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fourth Convolutional Layer\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "        # Fifth Convolutional Layer\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Dropout Layers\n",
    "        self.dropout1 = nn.Dropout(self.dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(self.dropout_prob)\n",
    "        self.dropout3 = nn.Dropout(self.dropout_prob)\n",
    "        self.dropout4 = nn.Dropout(self.dropout_prob)\n",
    "        # self.dropout5 = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        fc1_in_features = 256 * 8 * 25\n",
    "        self.fc1 = nn.Linear(in_features=fc1_in_features, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.fc5 = nn.Linear(in_features=32, out_features=5)\n",
    "\n",
    "        # Output activitation function\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Convolutional Block\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "         # Fourth Convolutional Block\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Fifth Convolutional Block\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Flatten for Fully Connected Layers\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443dd0da",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "028908f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib plots using https://stackoverflow.com/questions/37360568/python-organisation-of-3-subplots-with-matplotlib\n",
    "from torcheval.metrics.aggregation.auc import AUC\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Training Loop with Validation\n",
    "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer):\n",
    "    # Lists to store training and validation losses, and accuracies\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    train_AUCs = []\n",
    "    val_AUCs = []\n",
    "\n",
    "\n",
    "    metric = AUC()\n",
    "    \n",
    "    # Loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Set the model to training mode\n",
    "        y_true = torch.tensor([]).to(device)\n",
    "        y_true_num = torch.tensor([]).to(device)\n",
    "        y_pred = torch.tensor([]).to(device)\n",
    "\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Training loop\n",
    "        for images, labels in train_loader:\n",
    "            for image, label in zip(images,labels):\n",
    "                image, label = image.to(device), label.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(image)\n",
    "\n",
    "                y_true = torch.cat((y_true,label),0)\n",
    "                # y_true.add(label) # Store label\n",
    "                y_pred = torch.cat((y_pred,outputs.data),0)\n",
    "                # y_pred.add(outputs.data) # Store predictions\n",
    "\n",
    "                loss = criterion(outputs, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "                metric.update(outputs.data,label) # Update AUC\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                # print(torch.max(label,1).indices[0])\n",
    "                y_true_num = torch.cat((y_true_num,torch.tensor([torch.max(label,1).indices[0]]).to(device)),0)\n",
    "                # print(y_true_num)\n",
    "                outputs.data = torch.tensor([0.,0.,0.,0.,0.]).to(device)\n",
    "                outputs.data[predicted] = 1.\n",
    "                \n",
    "                total_train += label.size(0)\n",
    "                correct_train += (label == outputs.data).sum().item()//5\n",
    "\n",
    "\n",
    "                \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Calculate training AUC\n",
    "        train_AUC = metric.compute().item()\n",
    "        train_AUCs.append(train_AUC)\n",
    "\n",
    "        # Calculate training F1 score\n",
    "        train_f1 = multiclass_f1_score(y_pred,y_true_num,num_classes=5)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "\n",
    "        # Reset AUC metric\n",
    "        metric.reset()\n",
    "\n",
    "        y_true_val = torch.tensor([]).to(device)\n",
    "        y_true_num_val = torch.tensor([]).to(device)\n",
    "        y_pred_val = torch.tensor([]).to(device)\n",
    "\n",
    "        # Validation without gradient computation\n",
    "        with torch.no_grad():\n",
    "            for val_images, val_labels in val_loader:\n",
    "                for image, label in zip(val_images,val_labels):\n",
    "                    image, label = image.to(device), label.to(device)\n",
    "                    val_outputs = model(image)\n",
    "                    val_loss = criterion(val_outputs, label)\n",
    "                    total_val_loss += val_loss.item()\n",
    " \n",
    "                    metric.update(val_outputs.data,label) # Update AUC\n",
    "                    y_true_val = torch.cat((y_true,label),0)\n",
    "                    y_pred_val = torch.cat((y_pred_val,val_outputs.data),0)\n",
    "                    # print(val_outputs.data)\n",
    "\n",
    "                    _, predicted_val = torch.max(val_outputs.data, 1)\n",
    "                    y_true_num_val = torch.cat((y_true_num_val,torch.tensor([torch.max(label,1).indices[0]]).to(device)),0)\n",
    "                    # print(label)\n",
    "\n",
    "                    val_outputs.data = torch.tensor([0.,0.,0.,0.,0.]).to(device)\n",
    "                    val_outputs.data[predicted_val[0]] = 1.\n",
    "\n",
    "                    total_val += label.size(0)\n",
    "                    correct_val += (label == val_outputs.data).sum().item()//5\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        val_accuracy = correct_val / total_val\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Calculate validation AUC\n",
    "        val_AUC = metric.compute().item()\n",
    "        val_AUCs.append(val_AUC)\n",
    "\n",
    "        metric.reset()\n",
    "\n",
    "        # Calculate validation F1 score\n",
    "        val_f1 = multiclass_f1_score(y_pred_val,y_true_num_val,num_classes=5)\n",
    "        # Print progress every 10 epochs\n",
    "        # if (epoch + 1) % 10 == 0:\n",
    "        if True:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "                f'Train - loss: {avg_train_loss:.4f}, acc: {train_accuracy * 100:.2f}%, AUC: {train_AUC:.3f}, F1: {train_f1:.3f}; '\n",
    "                f'Validation - loss: {avg_val_loss:.4f}, acc: {val_accuracy * 100:.2f}%, AUC: {val_AUC:.3f}, F1: {val_f1:.3f}')\n",
    "\n",
    "    # Plotting the loss and accuracy over epochs\n",
    "    gs = gridspec.GridSpec(2, 2)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = plt.subplot(gs[0, 0]) # row 0, col 0\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    ax = plt.subplot(gs[0, 1]) # row 0, col 1\n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs[1, :]) # row 1, span all columns\n",
    "    plt.plot(train_AUCs, label='Training AUC')\n",
    "    plt.plot(val_AUCs, label='Validation AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a185fc2a",
   "metadata": {},
   "source": [
    "## Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df56ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.aggregation.auc import AUC\n",
    "\n",
    "# Setting Hyperparameters and Training the Model\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 25\n",
    "\n",
    "# Create an instance of the SimpleCNN model and move it to the specified device (GPU if available)\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "# Define the loss criterion (CrossEntropyLoss) and the optimizer (Adam) for training the model\n",
    "weights = torch.tensor([0.17768543, 0.1938815, 0.20993698, 0.14523569, 0.2732604]).to(device) # Weighted based on data availability (see 1 Data Analysis.ipynb)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model using the defined training function\n",
    "val_losses_simple = train_model(model, train_loader, val_loader, epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a13d83",
   "metadata": {},
   "source": [
    "## Regularization techniques\n",
    "\n",
    "one widely used regularization technique is called Drouppout. Iirc, it systematically deactivates some neurons during training to make the model more robust for when the test data doesn't have some parts of the expected pattern. (I've implmented that below, but if we decide to use something else, then that's also fine.)\n",
    "\n",
    "More possible techniques: \n",
    "- early stopping (would be nice if model overfits)\n",
    "- L1 or L2 regularization (makes weights smaller/less of them) (affects the los function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1905406",
   "metadata": {},
   "outputs": [],
   "source": [
    "##NN with dropout\n",
    "\n",
    "class SimpleCNN_with_dropout(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.5):\n",
    "        super(SimpleCNN_with_dropout, self).__init__()\n",
    "\n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Third Convolutional Layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fourth Convolutional Layer\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "        # Fifth Convolutional Layer\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        #Dropouts\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        self.dropout3 = nn.Dropout(dropout_prob)\n",
    "        self.dropout4 = nn.Dropout(dropout_prob)\n",
    "        self.dropout5 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        fc1_in_features = 256 * 8 * 25\n",
    "        self.fc1 = nn.Linear(in_features=fc1_in_features, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.fc5 = nn.Linear(in_features=32, out_features=5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Convolutional Block\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # Dropout applied after the first convolutional layer\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Dropout applied after the second convolutional layer\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Dropout applied after the third convolutional layer\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "         # Fourth Convolutional Block\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Dropout applied after the fourth convolutional layer\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        # Fifth Convolutional Block\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Dropout applied after the fifth convolutional layer\n",
    "        x = self.dropout5(x)\n",
    "        \n",
    "        # Flatten for Fully Connected Layers\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049e1fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_model() missing 1 required positional argument: '_metric'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model_d\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m val_losses_dropout \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: train_model() missing 1 required positional argument: '_metric'"
     ]
    }
   ],
   "source": [
    "#Train CNN with drouput\n",
    "epochs = 100\n",
    "model_d = SimpleCNN_with_dropout(dropout_prob=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_d.parameters(), lr=0.001)\n",
    "val_losses_dropout = train_model(model_d, train_loader, val_loader, epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0656dbb9",
   "metadata": {},
   "source": [
    "Why use the parameters we use?\n",
    "\n",
    "Adam optimizer\n",
    "- generally concidered the best optimizer\n",
    "- implements both momentum and adaptive learning rate\n",
    "    - adaptive learning rate: each parameter gets it's own learning rate which helps with finding the minimum for the cost function connected to that secific parameter\n",
    "    - momentum: accelerates convergence (finding the minimum faster) because it accumulates information about past gradients (ie: it makes the process faster) and also allows the model not to get stuck in local minima as often\n",
    "\n",
    "Hyperparameters for Adam\n",
    "- Beta for momentum: defalut\n",
    "- Beta for  average square gradient: defalut\n",
    "- initial learning rate: 0.001\n",
    "- hyperparameter tuning:\n",
    "    - none yet\n",
    "\n",
    "Regularization techniques\n",
    "- allows for beter generalizability\n",
    "    - network doesn't rely on specific neutrons too much\n",
    "- reduces overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
