{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e25d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb64e7f",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa5d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = torch.as_tensor(np.load(img_path))\n",
    "        image = image.unsqueeze(0)\n",
    "        label = one_hot(torch.as_tensor(self.img_labels.iloc[idx, 1]-1),5).unsqueeze(0).to(torch.float)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18fe84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = CustomImageDataset(\"preprocessed/val/labels.csv\", \"preprocessed/val/specs\")\n",
    "train_set = CustomImageDataset(\"preprocessed/train/labels.csv\", \"preprocessed/train/specs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4fa11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd85b8",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a2647",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "1. **Convolutional Layers:**\n",
    "   - `self.conv1`: The first convolutional layer takes the input (RGB images) and produces feature maps with 16 output channels. The kernel size is set to 3 and padding needs to be 1 to keep the input dimension.\n",
    "   - `self.conv2`: The second convolutional layer takes the output of the first after the activation funtion and pooling are applied and produces a feature map with 32 channels, and has the the same kernel size and padding (3 and 1).\n",
    "   - `self.conv3`: The third convolutional layer further increases the number of output channels to 64.\n",
    "\n",
    "2. **Activation and Pooling:**\n",
    "   - `self.relu`: Rectified Linear Unit (ReLU) activation function is applied after each convolutional layer to introduce non-linearity.\n",
    "   - `self.pool`: Max-pooling layer with a kernel size of 2 and a stride of 2 is used to downsample the spatial dimensions.\n",
    "\n",
    "3. **Fully Connected Layers:**\n",
    "   - `self.fc1`: The first fully connected layer takes the flattened output from the last convolutional layer and maps it to 64 units.\n",
    "   - `self.fc2`: The final fully connected layer maps the 64 units to the output space with 10 units, corresponding to the number of classes in CIFAR-10.\n",
    "\n",
    "### Forward Pass\n",
    "The `forward` method defines the forward pass of the model. It specifies how input data flows through the layers to produce the final output. Convolutional and pooling layers are followed by activation functions, and the fully connected layers provide the classification logits. Note that the same relu and pooling layers are used in several parts. That is ok as these layers do not have parameters and are only applying the same function to any input, so no separate layers are needed.\n",
    "\n",
    "This simple CNN serves as a starting point for image classification tasks and can be further customized or extended for more complex problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2af05",
   "metadata": {},
   "source": [
    "Channel = 1 \n",
    "\n",
    "Padding = 2 (because dimension is 2) #only use if moving your filter would move you outside the convolution, so it's not mandatory\n",
    "\n",
    "filter size = width, height and channels (depends on the input size)\n",
    "\n",
    "Stride = how much youÂ´d have overlap in your steps\n",
    "\n",
    "Input first layer = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "737c9bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e330ea54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device set to: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc92db29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.75734079e-05, 8.10304118e-05, 8.59528300e-05, ...,\n",
       "        2.56379972e-05, 3.38812315e-05, 1.03176928e-04],\n",
       "       [4.83323238e-05, 1.34489237e-04, 6.52731978e-05, ...,\n",
       "        2.19504233e-04, 3.46366840e-04, 2.22038609e-04],\n",
       "       [1.51195025e-04, 2.51585559e-04, 4.53107874e-04, ...,\n",
       "        2.88863026e-04, 5.89006930e-04, 3.79063480e-04],\n",
       "       ...,\n",
       "       [4.35408722e-08, 9.69061347e-08, 1.73314916e-07, ...,\n",
       "        1.13678993e-07, 1.68862286e-07, 1.28463967e-07],\n",
       "       [5.15285414e-08, 1.03113841e-07, 1.11438155e-07, ...,\n",
       "        1.39387069e-07, 2.19962430e-07, 1.03649207e-07],\n",
       "       [6.13314484e-08, 1.19523705e-07, 1.11718748e-07, ...,\n",
       "        1.14359693e-07, 1.10422150e-07, 5.54369883e-08]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(f\"preprocessed/train/specs/{os.listdir('preprocessed/train/specs')[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f560a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necesary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540c3fb",
   "metadata": {},
   "source": [
    "## CNN specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "168bce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Third Convolutional Layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fourth Convolutional Layer\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "        # Fifth Convolutional Layer\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        fc1_in_features = 256 * 8 * 25\n",
    "        self.fc1 = nn.Linear(in_features=fc1_in_features, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.fc5 = nn.Linear(in_features=32, out_features=5)\n",
    "\n",
    "        # Output activitation function\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Convolutional Block\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "         # Fourth Convolutional Block\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Fifth Convolutional Block\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Flatten for Fully Connected Layers\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443dd0da",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "028908f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop with Validation\n",
    "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer):\n",
    "    # Lists to store training and validation losses, and accuracies\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Training loop\n",
    "        for images, labels in train_loader:\n",
    "            for image, label in zip(images,labels):\n",
    "                image, label = image.to(device), label.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(image)\n",
    "                # print(outputs)\n",
    "                # print(label)\n",
    "                \n",
    "                loss = criterion(outputs, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_train += label.size(0)\n",
    "                correct_train += (predicted == label).sum().item()\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        # Validation without gradient computation\n",
    "        with torch.no_grad():\n",
    "            for val_images, val_labels in val_loader:\n",
    "                for image, label in zip(images,labels):\n",
    "                    image, label = image.to(device), label.to(device)\n",
    "                    val_outputs = model(image)\n",
    "                    val_loss = criterion(val_outputs, label)\n",
    "                    total_val_loss += val_loss.item()\n",
    "\n",
    "                    _, predicted_val = torch.max(val_outputs.data, 1)\n",
    "                    total_val += label.size(0)\n",
    "                    correct_val += (predicted_val == label).sum().item()\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        val_accuracy = correct_val / total_val\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Print progress every 10 epochs\n",
    "        # if (epoch + 1) % 10 == 0:\n",
    "        if True:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "                f'Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy * 100:.2f}%, '\n",
    "                f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
    "\n",
    "    # Plotting the loss and accuracy over epochs\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a185fc2a",
   "metadata": {},
   "source": [
    "## Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df56ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 52.9064, Training Accuracy: 399.84%, Validation Loss: 5.7145, Validation Accuracy: 400.00%\n",
      "Epoch [2/20], Training Loss: 52.8766, Training Accuracy: 400.00%, Validation Loss: 4.7145, Validation Accuracy: 400.00%\n",
      "Epoch [3/20], Training Loss: 52.8766, Training Accuracy: 400.00%, Validation Loss: 3.7145, Validation Accuracy: 400.00%\n",
      "Epoch [4/20], Training Loss: 52.8766, Training Accuracy: 400.00%, Validation Loss: 5.7145, Validation Accuracy: 400.00%\n",
      "Epoch [5/20], Training Loss: 52.8766, Training Accuracy: 400.00%, Validation Loss: 4.7145, Validation Accuracy: 400.00%\n",
      "Epoch [6/20], Training Loss: 52.8766, Training Accuracy: 400.00%, Validation Loss: 4.7145, Validation Accuracy: 400.00%\n",
      "Epoch [7/20], Training Loss: 52.8766, Training Accuracy: 400.00%, Validation Loss: 4.7145, Validation Accuracy: 400.00%\n",
      "Epoch [8/20], Training Loss: 52.8766, Training Accuracy: 400.00%, Validation Loss: 4.7145, Validation Accuracy: 400.00%\n",
      "Epoch [9/20], Training Loss: 52.8766, Training Accuracy: 400.00%, Validation Loss: 5.7145, Validation Accuracy: 400.00%\n"
     ]
    }
   ],
   "source": [
    "# Setting Hyperparameters and Training the Model\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 20\n",
    "\n",
    "# Create an instance of the SimpleCNN model and move it to the specified device (GPU if available)\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "# Define the loss criterion (CrossEntropyLoss) and the optimizer (Adam) for training the model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model using the defined training function\n",
    "val_losses_simple = train_model(model, train_loader, val_loader, epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a13d83",
   "metadata": {},
   "source": [
    "## Regularization techniques\n",
    "\n",
    "one widely used regularization technique is called Drouppout. Iirc, it systematically deactivates some neurons during training to make the model more robust for when the test data doesn't have some parts of the expected pattern. (I've implmented that below, but if we decide to use something else, then that's also fine.)\n",
    "\n",
    "More possible techniques: \n",
    "- early stopping (would be nice if model overfits)\n",
    "- L1 or L2 regularization (makes weights smaller/less of them) (affects the los function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1905406",
   "metadata": {},
   "outputs": [],
   "source": [
    "##NN with dropout\n",
    "\n",
    "class SimpleCNN_with_dropout(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.5):\n",
    "        super(SimpleCNN_with_dropout, self).__init__()\n",
    "\n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Third Convolutional Layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fourth Convolutional Layer\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "        # Fifth Convolutional Layer\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        #Dropouts\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        self.dropout3 = nn.Dropout(dropout_prob)\n",
    "        self.dropout4 = nn.Dropout(dropout_prob)\n",
    "        self.dropout5 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        fc1_in_features = 256 * 8 * 25\n",
    "        self.fc1 = nn.Linear(in_features=fc1_in_features, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.fc5 = nn.Linear(in_features=32, out_features=5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Convolutional Block\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # Dropout applied after the first convolutional layer\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Dropout applied after the second convolutional layer\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Dropout applied after the third convolutional layer\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "         # Fourth Convolutional Block\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Dropout applied after the fourth convolutional layer\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        # Fifth Convolutional Block\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Dropout applied after the fifth convolutional layer\n",
    "        x = self.dropout5(x)\n",
    "        \n",
    "        # Flatten for Fully Connected Layers\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a049e1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 1.6091, Training Accuracy: 300.40%, Validation Loss: 1.5952, Validation Accuracy: 0.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model_d\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m val_losses_dropout \u001b[38;5;241m=\u001b[39m train_model(model_d, train_loader, val_loader, epochs, criterion, optimizer)\n",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, epochs, criterion, optimizer)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# print(outputs)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# print(labels)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\hajko\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hajko\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[0;32m    290\u001b[0m     tensors,\n\u001b[0;32m    291\u001b[0m     grad_tensors_,\n\u001b[0;32m    292\u001b[0m     retain_graph,\n\u001b[0;32m    293\u001b[0m     create_graph,\n\u001b[0;32m    294\u001b[0m     inputs,\n\u001b[0;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hajko\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train CNN with drouput\n",
    "epochs = 100\n",
    "model_d = SimpleCNN_with_dropout(dropout_prob=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_d.parameters(), lr=0.001)\n",
    "val_losses_dropout = train_model(model_d, train_loader, val_loader, epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0656dbb9",
   "metadata": {},
   "source": [
    "Why use the parameters we use?\n",
    "\n",
    "Adam optimizer\n",
    "- generally concidered the best optimizer\n",
    "- implements both momentum and adaptive learning rate\n",
    "    - adaptive learning rate: each parameter gets it's own learning rate which helps with finding the minimum for the cost function connected to that secific parameter\n",
    "    - momentum: accelerates convergence (finding the minimum faster) because it accumulates information about past gradients (ie: it makes the process faster) and also allows the model not to get stuck in local minima as often\n",
    "\n",
    "Hyperparameters for Adam\n",
    "- Beta for momentum: defalut\n",
    "- Beta for  average square gradient: defalut\n",
    "- initial learning rate: 0.001\n",
    "- hyperparameter tuning:\n",
    "    - a\n",
    "\n",
    "Regularization techniques\n",
    "- allows for beter generalizability\n",
    "    - network doesn't rely on specific neutrons too much\n",
    "- reduces overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
