{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e25d716",
   "metadata": {},
   "source": [
    "batch_size = 64"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7cb64e7f",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa5d01c",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations_file = None, transform=None, target_transform=None):\n",
    "        self.annotations_file = annotations_file\n",
    "        self.img_labels = pd.read_csv(annotations_file) if (annotations_file != None) else None\n",
    "        self.img_dir = img_dir\n",
    "        self.file_names = os.listdir(img_dir)\n",
    "        self.file_names.sort()\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels) if self.annotations_file != None else len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.annotations_file != None:\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        else:\n",
    "            img_path = os.path.join(self.img_dir, self.file_names[idx])\n",
    "        image = torch.as_tensor(np.load(img_path))\n",
    "        image = image.unsqueeze(0)\n",
    "        # print(image[0].shape)\n",
    "        \n",
    "        label = one_hot(torch.as_tensor(self.img_labels.iloc[idx, 1]-1),5).unsqueeze(0).to(torch.float) if (self.annotations_file != None) else self.file_names[idx].split(\".\")[0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18fe84bc",
   "metadata": {},
   "source": [
    "val_set = CustomImageDataset(\"preprocessed/val/specs\",\"preprocessed/val/labels.csv\")\n",
    "train_set = CustomImageDataset(\"preprocessed/train/specs\", \"preprocessed/train/labels.csv\")\n",
    "test_set = CustomImageDataset(\"preprocessed_no_silence/test/specs\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4fa11fb",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8decb12f",
   "metadata": {},
   "source": [
    "next(iter(train_loader))[0].shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d02989de",
   "metadata": {},
   "source": [
    "np.load(\"preprocessed/all/specs/1f_1018_0.npy\").shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bacd85b8",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a2647",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "1. **Convolutional Layers:**\n",
    "   - `self.conv1`: The first convolutional layer takes the input (RGB images) and produces feature maps with 16 output channels. The kernel size is set to 3 and padding needs to be 1 to keep the input dimension.\n",
    "   - `self.conv2`: The second convolutional layer takes the output of the first after the activation funtion and pooling are applied and produces a feature map with 32 channels, and has the the same kernel size and padding (3 and 1).\n",
    "   - `self.conv3`: The third convolutional layer further increases the number of output channels to 64.\n",
    "\n",
    "2. **Activation and Pooling:**\n",
    "   - `self.relu`: Rectified Linear Unit (ReLU) activation function is applied after each convolutional layer to introduce non-linearity.\n",
    "   - `self.pool`: Max-pooling layer with a kernel size of 2 and a stride of 2 is used to downsample the spatial dimensions.\n",
    "\n",
    "3. **Fully Connected Layers:**\n",
    "   - `self.fc1`: The first fully connected layer takes the flattened output from the last convolutional layer and maps it to 64 units.\n",
    "   - `self.fc2`: The final fully connected layer maps the 64 units to the output space with 10 units, corresponding to the number of classes in CIFAR-10.\n",
    "\n",
    "### Forward Pass\n",
    "The `forward` method defines the forward pass of the model. It specifies how input data flows through the layers to produce the final output. Convolutional and pooling layers are followed by activation functions, and the fully connected layers provide the classification logits. Note that the same relu and pooling layers are used in several parts. That is ok as these layers do not have parameters and are only applying the same function to any input, so no separate layers are needed.\n",
    "\n",
    "This simple CNN serves as a starting point for image classification tasks and can be further customized or extended for more complex problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2af05",
   "metadata": {},
   "source": [
    "Channel = 1 \n",
    "\n",
    "Padding = 2 (because dimension is 2) #only use if moving your filter would move you outside the convolution, so it's not mandatory\n",
    "\n",
    "filter size = width, height and channels (depends on the input size)\n",
    "\n",
    "Stride = how much youÂ´d have overlap in your steps\n",
    "\n",
    "Input first layer = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "737c9bc8",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import os"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e330ea54",
   "metadata": {},
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device set to: {device}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc92db29",
   "metadata": {},
   "source": [
    "np.load(f\"preprocessed_no_silence/train/specs/{os.listdir('preprocessed_no_silence/train/specs')[1]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f560a81",
   "metadata": {},
   "source": [
    "# Import necesary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b540c3fb",
   "metadata": {},
   "source": [
    "## CNN specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168bce81",
   "metadata": {},
   "source": [
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self,dropout_prob=0.5):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Third Convolutional Layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.convRes1 = nn.Conv2d(in_channels=16, out_channels = 64, kernel_size=1)\n",
    "\n",
    "        # Fourth Convolutional Layer\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "        # Fifth Convolutional Layer\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Dropout Layers\n",
    "        self.dropout1 = nn.Dropout(self.dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(self.dropout_prob)\n",
    "        self.dropout3 = nn.Dropout(self.dropout_prob)\n",
    "        self.dropout4 = nn.Dropout(self.dropout_prob)\n",
    "        # self.dropout5 = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        \n",
    "        fc1_in_features = 25600 # 51200\n",
    "        self.fc1 = nn.Linear(in_features=fc1_in_features, out_features=256)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=64)\n",
    "        # self.fc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        # self.fc5 = nn.Linear(in_features=32, out_features=5)\n",
    "        self.fc6 = nn.Linear(in_features=64, out_features=5)\n",
    "\n",
    "        # Output activitation function\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Convolutional Block\n",
    "        # print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        # res = x\n",
    "        # x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        # x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        # print(x.shape)\n",
    "        # print(res.shape)\n",
    "        # res = self.convRes1(res)\n",
    "        # print(res.shape)\n",
    "        # x += res\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Fourth Convolutional Block\n",
    "        # x = self.dropout1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Fifth Convolutional Block\n",
    "        # x = self.dropout1(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Flatten for Fully Connected Layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        # x = self.fc4(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.dropout4(x)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        # x = self.fc5(x)but also, there's\n",
    "        # x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "443dd0da",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "028908f5",
   "metadata": {},
   "source": [
    "# Matplotlib plots using https://stackoverflow.com/questions/37360568/python-organisation-of-3-subplots-with-matplotlib\n",
    "from torcheval.metrics.aggregation.auc import AUC\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "import matplotlib.gridspec as gridspec\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training Loop with Validation\n",
    "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer):\n",
    "    # Lists to store training and validation losses, and accuracies\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    train_AUCs = []\n",
    "    val_AUCs = []    \n",
    "    train_f1_scores = []\n",
    "    val_f1_scores = []\n",
    "\n",
    "    metric = AUC(n_tasks=batch_size)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_weights = model.state_dict()\n",
    "    \n",
    "    # Loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Lists to store true labels and predictions for F1 score\n",
    "        all_train_labels = []\n",
    "        all_train_preds = []\n",
    "\n",
    "        # Training loop\n",
    "        # print(next(iter(train_loader)))\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move to GPU if available\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get model outputs\n",
    "            outputs = model(images)  # Pass the entire batch to the model\n",
    "            \n",
    "            # Calculate loss\n",
    "            # print(labels.view(-1))\n",
    "            labels = labels.reshape(len(labels),5)\n",
    "            labels_indices = torch.argmax(labels, dim=1)\n",
    "            # print(labels.shape)\n",
    "            # print(labels)\n",
    "            # print(outputs.shape)\n",
    "            loss = criterion(outputs, labels_indices)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            outputs_prob = F.softmax(outputs, dim=1)  # Apply softmax to get probabilities\n",
    "\n",
    "            # Update metrics\n",
    "            if len(images) == batch_size:\n",
    "                metric.update(outputs_prob, labels)  # Update AUC with probabilities\n",
    "        \n",
    "            for i in range(len(images)):\n",
    "                output_prob = outputs_prob[i]\n",
    "                label = labels[i]\n",
    "\n",
    "                # Get predicted classes\n",
    "                # print(output_prob)\n",
    "                _, predicted = torch.max(output_prob, dim=0)\n",
    "\n",
    "                # Store true labels and predictions for F1 score\n",
    "                all_train_labels.append(label.cpu())\n",
    "                all_train_preds.append(predicted.cpu())\n",
    "\n",
    "                # Store true labels and predictions\n",
    "                total_train += label.size(0)\n",
    "                correct_train += (predicted == label).sum().item()  # Count correct predictions\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Calculate training AUC\n",
    "        # print(metric.compute())\n",
    "        train_AUC = metric.compute()\n",
    "        train_AUCs.append(train_AUC)\n",
    "        # print(train_AUCs)\n",
    "        # Reset AUC metric for the next epoch\n",
    "        metric.reset()\n",
    "\n",
    "        # Calculate weighted F1 score\n",
    "        train_f1 = multiclass_f1_score(torch.stack(all_train_preds), torch.stack(all_train_labels).argmax(1), average='weighted', num_classes=5)\n",
    "        train_f1_scores.append(train_f1.item())\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        y_true_val = torch.tensor([]).to(device)\n",
    "        y_pred_val = torch.tensor([]).to(device)\n",
    "        \n",
    "        # Lists to store true labels and predictions for F1 score\n",
    "        all_val_labels = []\n",
    "        all_val_preds = []\n",
    "\n",
    "        # Validation without gradient computation\n",
    "        with torch.no_grad():\n",
    "            for val_images, val_labels in val_loader:\n",
    "                val_images, val_labels = val_images.to(device), val_labels.to(device)  # Move to GPU if available\n",
    "                \n",
    "                val_outputs = model(val_images)  # Pass the entire batch to the model\n",
    "\n",
    "                # Calculate loss\n",
    "                val_labels = val_labels.reshape(len(val_labels),5)\n",
    "                val_labels_indices = torch.argmax(val_labels, dim=1)\n",
    "                # print(labels.shape)\n",
    "                # print(labels)\n",
    "                # print(outputs.shape)\n",
    "                val_loss = criterion(val_outputs, val_labels_indices)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                # Convert logits to probabilities\n",
    "                val_outputs_prob = F.softmax(val_outputs, dim=1)  # Apply softmax to get probabilities\n",
    "\n",
    "                # Update metrics\n",
    "                if len(val_images) == batch_size:\n",
    "                    metric.update(val_outputs_prob, val_labels)  # Update AUC\n",
    "\n",
    "                # Get predicted classes\n",
    "                _, predicted_val = torch.max(val_outputs_prob, 1)\n",
    "\n",
    "                # Store true labels and predictions\n",
    "                total_val += val_labels.size(0)\n",
    "                for i in range(len(val_images)):\n",
    "                    val_output_prob = val_outputs_prob[i]\n",
    "                    val_label = val_labels[i]\n",
    "\n",
    "                    # Get predicted classes\n",
    "                    # print(output_prob)\n",
    "                    _, predicted_val = torch.max(val_output_prob, dim=0)\n",
    "\n",
    "                    all_val_labels.append(val_label.cpu())\n",
    "                    all_val_preds.append(predicted_val.cpu())\n",
    "\n",
    "                    # Store true labels and predictions\n",
    "                    total_val += val_label.size(0)\n",
    "                    # print(correct_val)\n",
    "                    # print(predicted_val.shape)\n",
    "                    # print(val_label.shape)\n",
    "                    correct_val += (predicted_val == val_label).sum().item()  # Count correct predictions\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        val_accuracy = correct_val / total_val\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Calculate validation AUC\n",
    "        val_AUC = metric.compute()\n",
    "        val_AUCs.append(val_AUC)\n",
    "\n",
    "        # Reset AUC metric for the next epoch\n",
    "        metric.reset()\n",
    "\n",
    "        # Calculate weighted F1 score for training\n",
    "        val_f1 = multiclass_f1_score(torch.stack(all_val_preds), torch.stack(all_val_labels).argmax(1), average='weighted', num_classes=5)\n",
    "        if val_f1 > best_f1:\n",
    "            best_weights = model.state_dict()\n",
    "            best_f1 = val_f1\n",
    "        val_f1_scores.append(val_f1.item())\n",
    "\n",
    "        # print(type(avg_train_loss), avg_train_loss)\n",
    "        # print(type(train_accuracy), train_accuracy)\n",
    "        # print(type(avg_val_loss), avg_val_loss)\n",
    "        # print(type(val_accuracy),val_accuracy)\n",
    "        # print(type(train_AUC), train_AUC.mean().item())\n",
    "        # print(type(val_AUC), val_AUC.mean().item())\n",
    "\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "              f'Train - loss: {avg_train_loss:.4f}, acc: {train_accuracy * 100:.2f}%, AUC: {train_AUC.mean().item():.3f}, F1: {train_f1:.3f}; '\n",
    "              f'Validation - loss: {avg_val_loss:.4f}, acc: {val_accuracy * 100:.2f}%, AUC: {val_AUC.mean().item():.3f}, F1: {val_f1:.3f}')\n",
    "    \n",
    "    # Plotting the loss, accuracy, AUC, and F1 score over epochs\n",
    "    gs = gridspec.GridSpec(3, 2)  # Create a 3x2 grid\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    # Plot Training and Validation Loss\n",
    "    ax = plt.subplot(gs[0, 0])  # row 0, col 0\n",
    "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "    plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Validation Accuracy\n",
    "    ax = plt.subplot(gs[0, 1])  # row 0, col 1\n",
    "    plt.plot(train_accuracies, label='Training Accuracy', color='blue')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # # Plot Training and Validation AUC\n",
    "    # ax = plt.subplot(gs[1, 0])  # row 1, col 0\n",
    "    # plt.plot([train_AUC.cpu() for train_AUC in train_AUCs], label='Training AUC', color='blue')\n",
    "    # plt.plot([val_AUC.cpu() for val_AUC in val_AUCs], label='Validation AUC', color='orange')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('AUC')\n",
    "    # plt.title('AUC over Epochs')\n",
    "    # plt.legend()\n",
    "\n",
    "    # Plot Training and Validation F1 Score\n",
    "    ax = plt.subplot(gs[1, 1])  # row 1, col 1\n",
    "    plt.plot(train_f1_scores, label='Training F1 Score', color='blue')\n",
    "    plt.plot(val_f1_scores, label='Validation F1 Score', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    print(f\"Best F1-score: {best_f1}\")\n",
    "    return val_losses"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a185fc2a",
   "metadata": {},
   "source": [
    "## Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4df56ea1",
   "metadata": {},
   "source": [
    "from torcheval.metrics.aggregation.auc import AUC\n",
    "\n",
    "# Setting Hyperparameters and Training the Model\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 75\n",
    "# Create an instance of the SimpleCNN model and move it to the specified device (GPU if available)\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "# Define the loss criterion (CrossEntropyLoss) and the optimizer (Adam) for training the model\n",
    "weights = torch.tensor([0.17768543*5, 0.1938815*5, 0.20993698*5, 0.14523569*5, 0.2732604*5]).to(device) # Weighted based on data availability (see 1 Data Analysis.ipynb)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model using the defined training function\n",
    "val_losses_simple = train_model(model, train_loader, val_loader, epochs, criterion, optimizer)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7e916ec6",
   "metadata": {},
   "source": [
    "## Predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694e89c",
   "metadata": {},
   "source": [
    "df_test = pd.DataFrame()\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    ids = []\n",
    "    pred_labels = []\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)  # Move to GPU if available\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = model(images)  # Pass the entire batch to the model\n",
    "        \n",
    "        test_outputs_prob = F.softmax(outputs, dim=1)\n",
    "\n",
    "        ids += labels\n",
    "        pred_labels += [output.item()+1 for output in test_outputs_prob.argmax(dim=1)]\n",
    "    # print(len(pred_labels), pred_labels)\n",
    "    df_test[\"Id\"] = ids\n",
    "    df_test[\"label\"] = pred_labels\n",
    "    # for output in df_test[\"label\"]:\n",
    "    #     print(output)\n",
    "df_test.to_csv(\"preprocessed/test/labels.csv\",index=False)\n",
    "df_test"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "28a13d83",
   "metadata": {},
   "source": [
    "## Regularization techniques\n",
    "\n",
    "one widely used regularization technique is called Drouppout. Iirc, it systematically deactivates some neurons during training to make the model more robust for when the test data doesn't have some parts of the expected pattern. (I've implmented that below, but if we decide to use something else, then that's also fine.)\n",
    "\n",
    "More possible techniques: \n",
    "- early stopping (would be nice if model overfits)\n",
    "- L1 or L2 regularization (makes weights smaller/less of them) (affects the los function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1905406",
   "metadata": {},
   "source": [
    "##NN with dropout\n",
    "\n",
    "class SimpleCNN_with_dropout(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.5):\n",
    "        super(SimpleCNN_with_dropout, self).__init__()\n",
    "\n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Third Convolutional Layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fourth Convolutional Layer\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "        # Fifth Convolutional Layer\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        #Dropouts\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        self.dropout3 = nn.Dropout(dropout_prob)\n",
    "        self.dropout4 = nn.Dropout(dropout_prob)\n",
    "        self.dropout5 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        fc1_in_features = 256 * 8 * 25\n",
    "        self.fc1 = nn.Linear(in_features=fc1_in_features, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.fc5 = nn.Linear(in_features=32, out_features=5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Convolutional Block\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # Dropout applied after the first convolutional layer\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Dropout applied after the second convolutional layer\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Dropout applied after the third convolutional layer\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "         # Fourth Convolutional Block\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Dropout applied after the fourth convolutional layer\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        # Fifth Convolutional Block\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Dropout applied after the fifth convolutional layer\n",
    "        x = self.dropout5(x)\n",
    "        \n",
    "        # Flatten for Fully Connected Layers\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049e1fb",
   "metadata": {},
   "source": [
    "#Train CNN with drouput\n",
    "epochs = 100\n",
    "model_d = SimpleCNN_with_dropout(dropout_prob=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_d.parameters(), lr=0.001)\n",
    "val_losses_dropout = train_model(model_d, train_loader, val_loader, epochs, criterion, optimizer)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0656dbb9",
   "metadata": {},
   "source": [
    "Why use the parameters we use?\n",
    "\n",
    "Adam optimizer\n",
    "- generally concidered the best optimizer\n",
    "- implements both momentum and adaptive learning rate\n",
    "    - adaptive learning rate: each parameter gets it's own learning rate which helps with finding the minimum for the cost function connected to that secific parameter\n",
    "    - momentum: accelerates convergence (finding the minimum faster) because it accumulates information about past gradients (ie: it makes the process faster) and also allows the model not to get stuck in local minima as often\n",
    "\n",
    "Hyperparameters for Adam\n",
    "- Beta for momentum: defalut\n",
    "- Beta for  average square gradient: defalut\n",
    "- initial learning rate: 0.001\n",
    "- hyperparameter tuning:\n",
    "    - none yet\n",
    "\n",
    "Regularization techniques\n",
    "- allows for beter generalizability\n",
    "    - network doesn't rely on specific neutrons too much\n",
    "- reduces overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_accent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
