{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed for sequential data processing. Unlike traditional feedforward neural networks, RNNs have the ability to capture and utilize information from previous time steps, making them suitable for tasks involving sequential dependencies.\n",
    "\n",
    "In this notebook, we will explore the fundamental concepts of RNNs using a simple implementation in PyTorch. We will create a custom RNN model, generate synthetic sequential data, and train the model to predict the next point in a sequence based on its previous values.\n",
    "\n",
    "## Custom RNN Model\n",
    "\n",
    "We start by defining a custom RNN model named `SimpleRNN`. This model implements a simple RNN using only linear layers and a non-linearity (tanh activation function) to capture sequential dependencies. The model is designed to predict the next point in a sequence.\n",
    "\n",
    "The parameters needed and the forward pass to be implemented in the `SimpleRNN` model can be described by the following equations:\n",
    "\n",
    "1. **Update Hidden State:**\n",
    "   \\[ $h_t = \\tanh(W_{xh} \\cdot x + W_{hh} \\cdot h_{\\text{prev}} + b)$ \\]\n",
    "\n",
    "   Where:\n",
    "   - \\( $h_t$ \\) is the current hidden state.\n",
    "   - \\( $x$ \\) is the input.\n",
    "   - \\( $h_{\\text{prev}}$ \\) is the previous hidden state.\n",
    "   - \\( $W_{xh}$ \\) is the weight matrix for the input.\n",
    "   - \\( $W_{hh}$ \\) is the weight matrix for the hidden state.\n",
    "   - \\($b$\\) is the bias term. Note that if you use Liner layers to implement this model, they already include biases if bias=True is used as an argument of the nn.Linear.\n",
    "\n",
    "2. **Output Prediction:**\n",
    "   \\[ $y_t = W_{hy} \\cdot h_t$ \\]\n",
    "\n",
    "   Where:\n",
    "   - \\( $y_t$ \\) is the output prediction.\n",
    "   - \\( $h_t$ \\) is the current hidden state.\n",
    "   - \\( $W_{hy}$ \\) is the weight matrix for the output.\n",
    "\n",
    "3. **Initial state $h_0$:**\n",
    "   The initial state $h_0$ needs to be explicitly creates as a parameter of the network. In the code this is done by defining the variable self.h0, which serves as the initial hidden state for the recurrent neural network (RNN).\n",
    "   This variables is created as: ```self.h0 = nn.Parameter(torch.rand([1, hidden_size]))```.\n",
    "   nn.Parameter is a class in PyTorch that is used to define learnable parameters within a PyTorch model. These parameters are automatically registered as model parameters, and their values can be updated during the training process through backpropagation. torch.rand([1, hidden_size]) initializes self.h0 with random values. Using random initialization helps break symmetries in the model and allows the learning process to start with a variety of hidden states.\n",
    "\n",
    "The forward function returns both the output prediction \\( y_t \\) and the updated hidden state \\( h_t \\).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the custom RNN model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.h0 = nn.Parameter(torch.rand([1, hidden_size]))\n",
    "\n",
    "        self.Wxh = nn.Linear(input_size, hidden_size)\n",
    "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Why = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Non-linearity (you can experiment with different activation functions)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        # Update hidden state using the RNN equations\n",
    "        h_t = self.tanh(self.Wxh(x) + self.Whh(h_prev))\n",
    "        \n",
    "        # Output prediction\n",
    "        y_t = self.Why(h_t)\n",
    "\n",
    "        return y_t, h_t\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Synthetic Data\n",
    "To demonstrate the capabilities of the RNN, we generate synthetic data by combining sine and cosine functions with random weights and frequencies. This synthetic data is then visualized to provide insight into the structure of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "# Generate synthetic data using a combination of sine and cosine functions\n",
    "def generate_synthetic_data(num_samples, seq_length, frequencies, weights):\n",
    "    data = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        to = np.random.rand(1)*2 \n",
    "        t = np.linspace(to, to + 2 * np.pi, seq_length+1)\n",
    "        sequence = np.zeros([seq_length+1, 1])\n",
    "        for freq, weight in zip(frequencies, weights):\n",
    "            sequence += weight * np.sin(freq * t) + weight * np.cos(freq * t)\n",
    "        data.append(sequence)\n",
    "\n",
    "    return np.array(data)\n",
    "\n",
    "# Hyperparameters\n",
    "num_samples = 300\n",
    "seq_length = 20\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "hidden_size = 8\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "frequencies = np.random.uniform(0.5, 2.5, size=4)\n",
    "weights = np.random.uniform(0.5, 2.0, size=4)\n",
    "synthetic_data = generate_synthetic_data(num_samples, seq_length, frequencies, weights)\n",
    "\n",
    "# Visualize a few sequences from the synthetic data\n",
    "num_sequences_to_visualize = 5\n",
    "\n",
    "# Visualize a few sequences from the synthetic data\n",
    "num_sequences_to_visualize = 4\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n",
    "\n",
    "for i in range(num_sequences_to_visualize):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    axes[row, col].plot(synthetic_data[i], label=f'Sequence {i + 1}')\n",
    "    axes[row, col].set_xlabel('Time Step')\n",
    "    axes[row, col].set_ylabel('Value')\n",
    "    axes[row, col].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the RNN\n",
    "The RNN is trained using the synthetic data. We employ Mean Squared Error (MSE) as the loss function and the Adam optimizer for parameter updates. The training loop iterates through epochs, updating the model parameters to minimize the prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "# Convert the data to PyTorch tensors\n",
    "X_tensor = torch.FloatTensor(synthetic_data[:, :-1]).unsqueeze(-1)\n",
    "y_tensor = torch.FloatTensor(synthetic_data[:, -1]).unsqueeze(-1)\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i in range(len(X_tensor)):\n",
    "        # Forward pass\n",
    "        h_prev = model.h0\n",
    "        output_seq = []\n",
    "\n",
    "        for t in range(X_tensor.shape[1]):\n",
    "            output, h_prev = model(X_tensor[i, t], h_prev)\n",
    "            output_seq.append(output)\n",
    "\n",
    "        output = output_seq[-1]\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(output, y_tensor[i])\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(X_tensor)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {average_loss:.8f}\")\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Predictions\n",
    "After training, we visualize the RNN's predictions compared to the ground truth for a few sequences. This provides a qualitative assessment of the model's ability to capture the underlying patterns in the data. When interpreting the results note that the prediction gets better the more data the network has seen; so, at the begining the predictions are not so good, but in the last time steps they become much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "# Visualize predictions\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(len(X_tensor)):\n",
    "        h_prev = torch.zeros(1, model.hidden_size)\n",
    "        prediction_seq = []\n",
    "\n",
    "        for t in range(X_tensor.shape[1]):\n",
    "            output, h_prev = model(X_tensor[i, t], h_prev)\n",
    "            prediction_seq.append(output.item())\n",
    "\n",
    "        predictions.append(prediction_seq)\n",
    "\n",
    "# Visualize predictions vs. ground truth in a 2 by 2 subplot layout\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for i in range(num_sequences_to_visualize):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    axes[row, col].plot(X_tensor[i].squeeze(), label='Ground Truth', linestyle='--', color='blue')\n",
    "    axes[row, col].plot(np.arange(1, seq_length+1), predictions[i], label='Predictions', linestyle='-', color='orange')\n",
    "    axes[row, col].set_xlabel('Time Step')\n",
    "    axes[row, col].set_ylabel('Value')\n",
    "    axes[row, col].legend()\n",
    "    axes[row, col].set_title(f'Sequence {i + 1} - Predictions vs. Ground Truth')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as an introductory guide to RNNs, illustrating their application in sequence modeling. Feel free to experiment with hyperparameters, explore different activation functions to deepen your understanding of recurrent neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
