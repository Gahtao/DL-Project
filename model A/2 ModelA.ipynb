{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:31:59.546539Z",
     "start_time": "2025-05-19T10:31:54.205489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from convenience import get_data_loaders"
   ],
   "id": "d5440cb773c050f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:31:59.566462Z",
     "start_time": "2025-05-19T10:31:59.546539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device set to: {device}\")"
   ],
   "id": "1571869311931956",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to: cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-19T10:31:59.580749Z",
     "start_time": "2025-05-19T10:31:59.566462Z"
    }
   },
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()        \n",
    "        \n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Third Convolutional Layer\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        fc1_in_features = 64 * 8 * 8\n",
    "        self.fc1 = nn.Linear(in_features=fc1_in_features, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Convolutional Block\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Flatten for Fully Connected Layers\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:31:59.603285Z",
     "start_time": "2025-05-19T10:31:59.580749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training Loop with Validation\n",
    "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer):\n",
    "    # Lists to store training and validation losses, and accuracies\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Training loop\n",
    "        for audios, labels in train_loader:\n",
    "            audios, labels = audios.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(audios)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        # Validation without gradient computation\n",
    "        with torch.no_grad():\n",
    "            for val_audio, val_labels in val_loader:\n",
    "                val_audio, val_labels = val_audio.to(device), val_labels.to(device)\n",
    "                val_outputs = model(val_audio)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                _, predicted_val = torch.max(val_outputs.data, 1)\n",
    "                total_val += val_labels.size(0)\n",
    "                correct_val += (predicted_val == val_labels).sum().item()\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        val_accuracy = correct_val / total_val\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "                f'Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy * 100:.2f}%, '\n",
    "                f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
    "\n",
    "    # Plotting the loss and accuracy over epochs\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return val_losses"
   ],
   "id": "714b9f7b9e0e9171",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:32:07.479215Z",
     "start_time": "2025-05-19T10:31:59.603285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setting Hyperparameters and Training the Model\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 40\n",
    "\n",
    "# Create an instance of the SimpleCNN model and move it to the specified device (GPU if available)\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "# Define the loss criterion (CrossEntropyLoss) and the optimizer (Adam) for training the model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loader, val_loader, test_loader, class_names = get_data_loaders(batch_size=256)\n",
    "\n",
    "# Train the model using the defined training function\n",
    "val_losses_simple = train_model(model, train_loader, val_loader, epochs, criterion, optimizer)\n",
    "\n"
   ],
   "id": "e3b971845c41831a",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "len() of a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m     10\u001B[39m criterion = nn.CrossEntropyLoss()\n\u001B[32m     11\u001B[39m optimizer = optim.Adam(model.parameters(), lr=\u001B[32m0.001\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m train_loader, val_loader, test_loader, class_names = \u001B[43mget_data_loaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m256\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# Train the model using the defined training function\u001B[39;00m\n\u001B[32m     16\u001B[39m val_losses_simple = train_model(model, train_loader, val_loader, epochs, criterion, optimizer)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\prive\\Uni\\DL-Project\\model A\\convenience.py:80\u001B[39m, in \u001B[36mget_data_loaders\u001B[39m\u001B[34m(batch_size)\u001B[39m\n\u001B[32m     78\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_data_loaders\u001B[39m(batch_size=\u001B[32m128\u001B[39m):\n\u001B[32m     79\u001B[39m     \u001B[38;5;66;03m# Load the full dataset\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m80\u001B[39m     df, loaded_audio, _ = \u001B[43mload_train_post_processing\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     82\u001B[39m     target = df[\u001B[33m'\u001B[39m\u001B[33maccent\u001B[39m\u001B[33m'\u001B[39m].values\n\u001B[32m     83\u001B[39m     features = torch.tensor(loaded_audio)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\prive\\Uni\\DL-Project\\model A\\convenience.py:74\u001B[39m, in \u001B[36mload_train_post_processing\u001B[39m\u001B[34m(path)\u001B[39m\n\u001B[32m     71\u001B[39m     sample_rates.add(sr)\n\u001B[32m     73\u001B[39m df_train.audio = loaded_audio\n\u001B[32m---> \u001B[39m\u001B[32m74\u001B[39m df_train[\u001B[33m\"\u001B[39m\u001B[33mlength\u001B[39m\u001B[33m\"\u001B[39m] = [\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mloaded_audio\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m / \u001B[32m16000\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(df_train))]\n\u001B[32m     75\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m df_train, loaded_audio, sample_rates\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\DL-Project\\Lib\\site-packages\\torch\\_tensor.py:1163\u001B[39m, in \u001B[36mTensor.__len__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1161\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(Tensor.\u001B[34m__len__\u001B[39m, (\u001B[38;5;28mself\u001B[39m,), \u001B[38;5;28mself\u001B[39m)\n\u001B[32m   1162\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dim() == \u001B[32m0\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1163\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mlen() of a 0-d tensor\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1164\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch._C._get_tracing_state():\n\u001B[32m   1165\u001B[39m     warnings.warn(\n\u001B[32m   1166\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mUsing len to get tensor shape might cause the trace to be incorrect. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1167\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mRecommended usage would be tensor.shape[0]. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1171\u001B[39m         stacklevel=\u001B[32m2\u001B[39m,\n\u001B[32m   1172\u001B[39m     )\n",
      "\u001B[31mTypeError\u001B[39m: len() of a 0-d tensor"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
